{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from httpx import AsyncClient\n",
    "import re\n",
    "import json\n",
    "import asyncio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "module_path = os.path.abspath(os.path.join('../transfermarket/'))\n",
    "sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=AsyncClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_delay = 0\n",
    "\n",
    "async def delayed_fetch(url:str,match_date:str,sleep_time_seconds:int = 3):\n",
    "    global next_delay\n",
    "    # need to increase the delay on each api request \n",
    "    # (since they all start the timers simulataneously)\n",
    "    next_delay += sleep_time_seconds\n",
    "    await asyncio.sleep(next_delay)\n",
    "    \n",
    "    print(\".\", end=\"\")\n",
    "    fetch = await client.get(url)\n",
    "    # need to keep track of which match this is so we can add metadata\n",
    "    # e.g. opponent name, match result\n",
    "    return match_date, fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from httpx import Response\n",
    "\n",
    "nunez_player_id = \"4d77b365\"\n",
    "\n",
    "async def get_match_data(player_id:str, season:str = '2022-2023')-> dict[str, dict[str,str]]:\n",
    "    match_summary_url = f\"https://fbref.com/en/players/{player_id}/matchlogs/{season}/summary/\"\n",
    "\n",
    "    match_page = await client.get(match_summary_url)\n",
    "    matches = pd.read_html(match_page.content ,extract_links=\"all\")[0].iloc[:, [0,4,5,6,7,-1]]\n",
    "    matches.columns = [(col[1][0]) for col in matches.columns] #type:ignore\n",
    "    # remove links for everything but match report\n",
    "    matches.iloc[:, :-1] = matches.iloc[:, :-1].applymap(lambda x:x[0])\n",
    "    matches.iloc[:, -1] = matches.iloc[:, -1].apply(lambda x:x[-1])\n",
    "\n",
    "    match_dates:dict[str, dict] = matches.dropna().set_index('Date').to_dict('index') #type:ignore\n",
    "    return match_dates\n",
    "\n",
    "async def get_all_shots(match_dates:dict[str, dict[str,str]]):\n",
    "    fetch_coros = []\n",
    "    next_delay = 0\n",
    "\n",
    "    async def delayed_fetch(url:str,match_date:str,sleep_time_seconds:int = 3)->tuple[str,Response]:\n",
    "        nonlocal next_delay\n",
    "        # need to increase the delay on each api request \n",
    "        # (since they all start the timers simulataneously)\n",
    "        next_delay += sleep_time_seconds\n",
    "        await asyncio.sleep(next_delay)\n",
    "        \n",
    "        print(\".\", end=\"\")\n",
    "        fetch = await client.get(url)\n",
    "        # need to keep track of which match this is so we can add metadata\n",
    "        # e.g. opponent name, match result\n",
    "        return match_date, fetch\n",
    "\n",
    "    for date, match_details in match_dates.items():\n",
    "        match_url = match_details['Match Report']\n",
    "        fetch_coros.append(delayed_fetch(f\"https://fbref.com/{match_url}\", date))\n",
    "\n",
    "    match_pages = await asyncio.gather(*fetch_coros)\n",
    "    return match_pages\n",
    "\n",
    "def process_shot_data(match_pages:tuple[str,Response]):\n",
    "    match_data =[]\n",
    "    for match_date, data in match_pages:\n",
    "        res_dfs = pd.read_html(data.content)\n",
    "\n",
    "        longest_xg_df = 0\n",
    "        xg_df = None\n",
    "        #we need to find xg table with most entries (to ensure we arent picking a filtered table)\n",
    "        for df in res_dfs:\n",
    "            try:\n",
    "                if 'xG' in df.droplevel(level=0, axis=1).columns:\n",
    "                    if len(df) > longest_xg_df:\n",
    "                        longest_xg_df = len(df)\n",
    "                        xg_df = df\n",
    "            except ValueError:\n",
    "                # we cant drop level so this isnt an xg table\n",
    "                continue\n",
    "    \n",
    "        if xg_df is None:\n",
    "            # no xg data available for this game\n",
    "            continue\n",
    "        shot_data = xg_df.iloc[:,[0,1,3,5] ]\n",
    "        shot_data= shot_data.droplevel(0, axis=1)\n",
    "        shot_data['date'] = match_date\n",
    "        match_data.append(shot_data)\n",
    "        \n",
    "    all_matches = pd.concat(match_data)\n",
    "    return all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabbing salah data\n",
      "2020-2021\n",
      ".....................................................2021-2022\n",
      "..................................................................2022-2023\n",
      "............................"
     ]
    }
   ],
   "source": [
    "# player_ids = {\n",
    "#     \"kane\":\"21a66f6a\", \n",
    "#     \"haaland\":\"1f44ac21\", \n",
    "#     \"firmino\": \"4c370d81\", \n",
    "#     \"salah\": \"e342ad68\", \n",
    "#     \"nunez\": \"4d77b365\",\n",
    "#     \"gakpo\": \"1971591f\",\n",
    "#     \"jota\": \"178ae8f8\", \n",
    "#     \"diaz\":\"4a1a9578\"\n",
    "#     }\n",
    "\n",
    "player_ids = {\"salah\": \"e342ad68\"}\n",
    "\n",
    "for player, player_id in player_ids.items():\n",
    "    print(f\"grabbing {player} data\")\n",
    "    shot_data = []\n",
    "    match_data = []\n",
    "\n",
    "    for season in [\"2020-2021\",\"2021-2022\",\"2022-2023\"]:\n",
    "        print(season)\n",
    "        matches = await get_match_data(player_id, season=season)\n",
    "        match_data.append(matches)\n",
    "        shots = await get_all_shots(matches)\n",
    "        processed_shots = process_shot_data(shots)\n",
    "        shot_data.append(processed_shots)\n",
    "\n",
    "    all_shot_data = pd.concat(shot_data).to_csv(f\"{player}_shot_data.csv\")\n",
    "    all_match_data = pd.concat([pd.DataFrame.from_dict(match, orient='index') for match in match_data]).to_csv(f\"{player}_match_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
